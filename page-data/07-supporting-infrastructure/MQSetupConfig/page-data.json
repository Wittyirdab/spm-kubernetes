{"componentChunkName":"component---src-pages-07-supporting-infrastructure-mq-setup-config-mdx","path":"/07-supporting-infrastructure/MQSetupConfig/","result":{"pageContext":{"frontmatter":{"title":"Configuring MQ on a VM","description":"Configuring MQ on a VM"},"relativePagePath":"/07-supporting-infrastructure/MQSetupConfig.mdx","titleType":"page","MdxNode":{"id":"9a57688d-6f70-5f5a-844b-acf18f52fbe1","children":[],"parent":"cd1be11d-71c0-53c9-a05a-0ad61a4762ba","internal":{"content":"---\ntitle: Configuring MQ on a VM\ndescription: Configuring MQ on a VM\n\n---\n\nTo support Kubernetes, changes to SPM were required.\nFor more information on these changes, see\n[Kubernetes Architecture](https://www.ibm.com/support/knowledgecenter/SS8S5A_7.0.10/com.ibm.curam.wlp.doc/Kubernetes/c_KubArchitecture.html) section in\nthe Knowledge Center.\n\nWhen SPM is containerized on Kubernetes, it uses IBM® MQ to manage JMS messages for Cúram Deferred Processes and Cúram Workflows. IBM MQ is a queue managing service from the IBM stack. For more information about MQ, see [About IBM MQ](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q001010_.htm).\n\nIf MQ fails, certain functionality will be unusable. This includes and is not limited to, creation of a case and creation of an application.\n\nThe MQ Cluster set up requires two queue managers nodes, with one active/primary queue manager, and one standby/secondary queue manager.\n\nSocial Program Management supports only IBM MQ LTS on a VM. The steps below outline how to do this. In this runbook we will outline the steps to create:\n\n* [Queues](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003090_.htm)\n* [Listeners](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003300_.htm)\n* [Channels](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003220_.htm)\n* [Topics](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003320_.htm)\n\n<InlineNotification>\n\n**Note:** The MQ version for this runbook verification is  9.1.0 LTS.\n\n</InlineNotification>\n\nFor the runbook, two standalone VMs were used as MQ nodes.\n\n### Queue manager names\n\nFor runbook configuration, the following naming conversion was used throughout the MQ setup: QM_NamingConvention_AppName. This must be unique, but ensure you change the commands used on this page accordingly.\n\n**Queue Name:**\n\n* QM_minikube_curam\n\n**Channel Name:** This value should be all capitals\n\nCHL_NamingConvention_AppName\n\n* CHL_MINIKUBE_CURAM\n\n**Listeners Name:** This value should be all capitals\n\nLS_NamingConvention_AppName\n\n* LS_MINIKUBE_CURAM\n\n## MQ stages\n\nOn both MQ nodes run the following command as root:\n\n```shell\nsu - mqm # Changing user into mqm\nexport PATH=/opt/mqm/inst1/bin:$PATH\n```\n\n<InlineNotification kind=\"warning\">\n\n**Important!**\n\nRun the export PATH command on both MQ nodes, this command will be used in further commands in the runbook.\n\n</InlineNotification>\n\n### Shared storage\n\nCreate the shared storage for our nodes.\n\n<InlineNotification>\n\n**Note:** Run the commands as root.\n\n</InlineNotification>\n\nOn the shared node run the following commands:\n\n```shell\nmkdir -p /MQHA/logs\nmkdir -p /MQHA/qmgrs\nmkdir -p /MQHA/scratch\nuseradd mqha -s /sbin/nologin\nchown -R mqha:mqha /MQHA/*\n```\n\nVerify that the UID and GUID match the owner ID by running the following command:\n\n```shell\necho \"/MQHA  MQ.FQDN(rw,sync,no_wdelay,fsid=0,anonuid=1001,anongid=1001)\" >> /etc/exports\n```\n\nStart and enable both the nfs service and rpcbind service by running the following commands:\n\n```shell\nsystemctl start nfs-server.service\nsystemctl enable nfs-server.service\nsystemctl start rpcbind\nsystemctl enable rpcbind\n```\n\nOn MQ nodes run the following commands:\n\n<InlineNotification>\n\n**Note:** Commands to be run as root.\n\n</InlineNotification>\n\n```shell\necho \"SHAREDNODEADDRESS:/MQHA  /MQHA  nfs  defaults  0 0\" >> /etc/fstab\nsystemctl start rpcbind\nsystemctl enable rpcbind\nmkdir -p /MQHA\nchmod 1777 /MQHA #Check permissions\nmount /MQHA\n```\n\n### Create QMs\n\nWhen creating the queue, start on the secondary node first then move to the primary node.\n\nOn the secondary MQ node, run the following commands:\n\n```shell\ncrtmqm -ld /MQHA/logs -md /MQHA/qmgrs QM_minikube_curam\ndspmqinf -o command QM_minikube_curam\n```\n\nSave the output of the above command. It should look like the following.\n\n```shell\naddmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam\n```\n\nWait for /MQHA/qmgrs/QM_minikube_curam/qm.ini to appear on other node\n\nOn the primary MQ node run the following commands:\n\n```shell\naddmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam\nstrmqm -x QM_minikube_curam\n```\n\nOn the secondary MQ node run the following command:\n\n```shell\nstrmqm -x QM_minikube_curam\n```\n\n### Create queues\n\nOn the primary MQ node run the following commands:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE QLOCAL(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWENACTMENT) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWACTIVITY) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.DPERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.DPENACTMENT) BOTHRESH(5) BOQNAME(QN.DPERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nALTER QMGR CHLAUTH(DISABLED)\nALTER QMGR DEADQ(QN.CURAMDEADMESSAGEQUEUE)\nEOS\n```\n\n### Create listeners\n\nOn the primary MQ node run the following commands:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE LISTENER (LS_MINIKUBE_CURAM) TRPTYPE (TCP) CONTROL (QMGR) PORT (1414)\nSTART LISTENER (LS_MINIKUBE_CURAM)\nEOS\n```\n\n### Create channels\n\nOn the primary MQ node run the following command:\n\n* Enter your MQ node names into the commands below\n\n<InlineNotification>\n\n**Note:** CERTLABL expects the value to be lower case ibmwebspheremq + Queue Name\nFor this example it will be ibmwebspheremqqm_minikube_curam\n\n</InlineNotification>\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(SVRCONN)  TRPTYPE(TCP) MCAUSER('mqm') SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256)  CERTLABL ('ibmwebspheremqqm_minikube_curam') SSLCAUTH (OPTIONAL) REPLACE\nDEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(CLNTCONN) TRPTYPE(TCP) CONNAME('Node1(1414),Node2(1414)') QMNAME(QM_minikube_curam) SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256) CERTLABL ('ibmwebspheremqqm_minikube_curam') REPLACE\nEOS\n```\n\n### Create topics\n\nOn the primary MQ node run the following command:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE TOPIC (CURAMCACHEINVALIDATIONTOPIC) TOPICSTR (CURAMCACHEINVALIDATIONTOPIC)\nALTER QMGR CONNAUTH('CHECK.PWD')\nDEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)\nEOS\n```\n\n### Configure security\n\nThe configuration of security is in four parts\n\n* Setting the object type.\n* Creating the keystore and certs.\n* Updating the certs on both nodes.\n* Refreshing security settings.\n\n<InlineNotification>\n\n**Note:** The application pods run as a non-root user (default). This non-root user must exist on both MQ nodes.\n\n</InlineNotification>\n\nOn the secondary MQ node run the following command:\n\n```shell\nuseradd -g 0 -M default && usermod -L default\n```\n\nOn the primary MQ node run the following commands:\n\n```shell\nuseradd -g 0 -M default && usermod -L default\nrunmqsc QM_minikube_curam <<-EOS\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPENACTMENT') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPERROR') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWACTIVITY') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWENACTMENT') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWERROR') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.CURAMDEADMESSAGEQUEUE') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(LISTENER) PROFILE('LS_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(CHANNEL) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(CLNTCONN) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(TOPIC) PROFILE('CURAMCACHEINVALIDATIONTOPIC') PRINCIPAL('default') AUTHADD(ALL)\nEOS\n```\n\n```shell\nrunmqckm -keydb -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -type cms -pw Passw0rd -stash\nrunmqakm -cert -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -size 2048 -dn \"CN=QM_minikube_curam,O=IBM,C=US\" -x509version 3 -expire 365 -sig_alg SHA1WithRSA\nrunmqakm -cert -extract -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.arm\nrunmqakm -cert -export -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -target_type pkcs12 -target_pw Passw0rd\n```\n\n```shell\nopenssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -nocerts -nodes | sed -ne '/-BEGIN PRIVATE KEY-/,/-END PRIVATE KEY-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.key\nopenssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -clcerts -nokeys | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.crt\n```\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nALTER QMGR CONNAUTH('CHECK.PWD')\nDEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)\nREFRESH SECURITY TYPE(SSL)\nREFRESH SECURITY TYPE(AUTHSERV)\nREFRESH SECURITY TYPE(CONNAUTH)\nEOS\n```\n\nAfter these stages have been run MQ should be configured.\n\n### Clean up QMs/channels/listeners\n\nUsed these steps if you are reconfiguring MQ or cleaning up MQ.\n\nOn both MQ nodes run the following commands:\n\n```shell\nendmqm -w QM_minikube_curam\ndltmqm QM_minikube_curam\nrmvmqinf QM_minikube_curam\n```\n\nOn either MQ node run the following commands:\n\n```shell\nrm -rf /MQHA/qmgrs/**\nrm -rf /MQHA/logs/**\nrm -rf /MQHA/scratch\nendmqm -w QM_minikube_curam\ndltmqm QM_minikube_curam\nrmvmqinf QM_minikube_curam\n```\n\n### MQ on OpenShift\n\n#### Stateful Sets\n\nIf a highly available MQ cluster is desired, a **Stateful Set** can be used. The stateful set used for SPM contains two identical\npods, one active pod and one standby pod. If the active pod goes down, the standby pod is moved into the active role and a new pod is rescheduled in standby mode.\nThis occurs seamlessly, with persistent storage allowing for minimal downtime. The Stateful Set used in SPM requires several values that must be configured prior to\ndeployment. These values are those located under the MQ `multiInstance` section within the relevant values file. There, NFS or Ceph can be chosen as the desired\nmulti-instance delivery method.\n\n* **NFS** - In order to deploy with NFS, an NFS server and NFS folder must be available and configured; the supplementalGroups may need to be provided depending on the NFS server security setup.\n* **Ceph** - In order to deploy with Ceph, the desired Storage Class must be provided.\n\n#### Persistent Volumes & Persistent Volume Claims\n\nA **PersistentVolume** (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.\nIt is a resource in the cluster just like a node is a cluster resource. A **PersistentVolumeClaim** (PVC) is a request for storage by a user. It is similar to\na Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific\nsize and access modes.\n\nWhen using NFS as the desired multi-instance method, the PV and PVCs must be configured by the user. Within the PVs, the NFS IP and NFS folder must be provided.\nIn the PV, a `claimRef` can be defined in order to ensure that the correct PVC matches with the correct PV. The templates provided also contain labels, which can\nalso be used to ensure correct coupling.\n\nIf using Ceph, the PVs are dynamically configured. Therefore, no further configuration is required.\n","type":"Mdx","contentDigest":"a41b20f4f2878f04159d93f7224153b5","counter":147,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Configuring MQ on a VM","description":"Configuring MQ on a VM"},"exports":{},"rawBody":"---\ntitle: Configuring MQ on a VM\ndescription: Configuring MQ on a VM\n\n---\n\nTo support Kubernetes, changes to SPM were required.\nFor more information on these changes, see\n[Kubernetes Architecture](https://www.ibm.com/support/knowledgecenter/SS8S5A_7.0.10/com.ibm.curam.wlp.doc/Kubernetes/c_KubArchitecture.html) section in\nthe Knowledge Center.\n\nWhen SPM is containerized on Kubernetes, it uses IBM® MQ to manage JMS messages for Cúram Deferred Processes and Cúram Workflows. IBM MQ is a queue managing service from the IBM stack. For more information about MQ, see [About IBM MQ](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q001010_.htm).\n\nIf MQ fails, certain functionality will be unusable. This includes and is not limited to, creation of a case and creation of an application.\n\nThe MQ Cluster set up requires two queue managers nodes, with one active/primary queue manager, and one standby/secondary queue manager.\n\nSocial Program Management supports only IBM MQ LTS on a VM. The steps below outline how to do this. In this runbook we will outline the steps to create:\n\n* [Queues](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003090_.htm)\n* [Listeners](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003300_.htm)\n* [Channels](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003220_.htm)\n* [Topics](https://www.ibm.com/support/knowledgecenter/SSFKSJ_9.1.0/com.ibm.mq.pro.doc/q003320_.htm)\n\n<InlineNotification>\n\n**Note:** The MQ version for this runbook verification is  9.1.0 LTS.\n\n</InlineNotification>\n\nFor the runbook, two standalone VMs were used as MQ nodes.\n\n### Queue manager names\n\nFor runbook configuration, the following naming conversion was used throughout the MQ setup: QM_NamingConvention_AppName. This must be unique, but ensure you change the commands used on this page accordingly.\n\n**Queue Name:**\n\n* QM_minikube_curam\n\n**Channel Name:** This value should be all capitals\n\nCHL_NamingConvention_AppName\n\n* CHL_MINIKUBE_CURAM\n\n**Listeners Name:** This value should be all capitals\n\nLS_NamingConvention_AppName\n\n* LS_MINIKUBE_CURAM\n\n## MQ stages\n\nOn both MQ nodes run the following command as root:\n\n```shell\nsu - mqm # Changing user into mqm\nexport PATH=/opt/mqm/inst1/bin:$PATH\n```\n\n<InlineNotification kind=\"warning\">\n\n**Important!**\n\nRun the export PATH command on both MQ nodes, this command will be used in further commands in the runbook.\n\n</InlineNotification>\n\n### Shared storage\n\nCreate the shared storage for our nodes.\n\n<InlineNotification>\n\n**Note:** Run the commands as root.\n\n</InlineNotification>\n\nOn the shared node run the following commands:\n\n```shell\nmkdir -p /MQHA/logs\nmkdir -p /MQHA/qmgrs\nmkdir -p /MQHA/scratch\nuseradd mqha -s /sbin/nologin\nchown -R mqha:mqha /MQHA/*\n```\n\nVerify that the UID and GUID match the owner ID by running the following command:\n\n```shell\necho \"/MQHA  MQ.FQDN(rw,sync,no_wdelay,fsid=0,anonuid=1001,anongid=1001)\" >> /etc/exports\n```\n\nStart and enable both the nfs service and rpcbind service by running the following commands:\n\n```shell\nsystemctl start nfs-server.service\nsystemctl enable nfs-server.service\nsystemctl start rpcbind\nsystemctl enable rpcbind\n```\n\nOn MQ nodes run the following commands:\n\n<InlineNotification>\n\n**Note:** Commands to be run as root.\n\n</InlineNotification>\n\n```shell\necho \"SHAREDNODEADDRESS:/MQHA  /MQHA  nfs  defaults  0 0\" >> /etc/fstab\nsystemctl start rpcbind\nsystemctl enable rpcbind\nmkdir -p /MQHA\nchmod 1777 /MQHA #Check permissions\nmount /MQHA\n```\n\n### Create QMs\n\nWhen creating the queue, start on the secondary node first then move to the primary node.\n\nOn the secondary MQ node, run the following commands:\n\n```shell\ncrtmqm -ld /MQHA/logs -md /MQHA/qmgrs QM_minikube_curam\ndspmqinf -o command QM_minikube_curam\n```\n\nSave the output of the above command. It should look like the following.\n\n```shell\naddmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam\n```\n\nWait for /MQHA/qmgrs/QM_minikube_curam/qm.ini to appear on other node\n\nOn the primary MQ node run the following commands:\n\n```shell\naddmqinf -s QueueManager -v Name=QM_minikube_curam -v Directory=QM_minikube_curam -v Prefix=/var/mqm -v DataPath=/MQHA/qmgrs/QM_minikube_curam\nstrmqm -x QM_minikube_curam\n```\n\nOn the secondary MQ node run the following command:\n\n```shell\nstrmqm -x QM_minikube_curam\n```\n\n### Create queues\n\nOn the primary MQ node run the following commands:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE QLOCAL(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWENACTMENT) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.WORKFLOWACTIVITY) BOTHRESH(5) BOQNAME(QN.WORKFLOWERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.DPERROR) BOTHRESH(5) BOQNAME(QN.CURAMDEADMESSAGEQUEUE) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nDEFINE QLOCAL(QN.DPENACTMENT) BOTHRESH(5) BOQNAME(QN.DPERROR) CLWLUSEQ (ANY) DEFBIND (NOTFIXED)\nALTER QMGR CHLAUTH(DISABLED)\nALTER QMGR DEADQ(QN.CURAMDEADMESSAGEQUEUE)\nEOS\n```\n\n### Create listeners\n\nOn the primary MQ node run the following commands:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE LISTENER (LS_MINIKUBE_CURAM) TRPTYPE (TCP) CONTROL (QMGR) PORT (1414)\nSTART LISTENER (LS_MINIKUBE_CURAM)\nEOS\n```\n\n### Create channels\n\nOn the primary MQ node run the following command:\n\n* Enter your MQ node names into the commands below\n\n<InlineNotification>\n\n**Note:** CERTLABL expects the value to be lower case ibmwebspheremq + Queue Name\nFor this example it will be ibmwebspheremqqm_minikube_curam\n\n</InlineNotification>\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(SVRCONN)  TRPTYPE(TCP) MCAUSER('mqm') SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256)  CERTLABL ('ibmwebspheremqqm_minikube_curam') SSLCAUTH (OPTIONAL) REPLACE\nDEFINE CHANNEL(CHL_MINIKUBE_CURAM) CHLTYPE(CLNTCONN) TRPTYPE(TCP) CONNAME('Node1(1414),Node2(1414)') QMNAME(QM_minikube_curam) SSLCIPH (TLS_RSA_WITH_AES_128_CBC_SHA256) CERTLABL ('ibmwebspheremqqm_minikube_curam') REPLACE\nEOS\n```\n\n### Create topics\n\nOn the primary MQ node run the following command:\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nDEFINE TOPIC (CURAMCACHEINVALIDATIONTOPIC) TOPICSTR (CURAMCACHEINVALIDATIONTOPIC)\nALTER QMGR CONNAUTH('CHECK.PWD')\nDEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)\nEOS\n```\n\n### Configure security\n\nThe configuration of security is in four parts\n\n* Setting the object type.\n* Creating the keystore and certs.\n* Updating the certs on both nodes.\n* Refreshing security settings.\n\n<InlineNotification>\n\n**Note:** The application pods run as a non-root user (default). This non-root user must exist on both MQ nodes.\n\n</InlineNotification>\n\nOn the secondary MQ node run the following command:\n\n```shell\nuseradd -g 0 -M default && usermod -L default\n```\n\nOn the primary MQ node run the following commands:\n\n```shell\nuseradd -g 0 -M default && usermod -L default\nrunmqsc QM_minikube_curam <<-EOS\nSET AUTHREC OBJTYPE(QMGR) PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPENACTMENT') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.DPERROR') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWACTIVITY') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWENACTMENT') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.WORKFLOWERROR') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(QUEUE) PROFILE('QN.CURAMDEADMESSAGEQUEUE') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(LISTENER) PROFILE('LS_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(CHANNEL) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(CLNTCONN) PROFILE('CHL_MINIKUBE_CURAM') PRINCIPAL('default') AUTHADD(ALL)\nSET AUTHREC OBJTYPE(TOPIC) PROFILE('CURAMCACHEINVALIDATIONTOPIC') PRINCIPAL('default') AUTHADD(ALL)\nEOS\n```\n\n```shell\nrunmqckm -keydb -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -type cms -pw Passw0rd -stash\nrunmqakm -cert -create -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -size 2048 -dn \"CN=QM_minikube_curam,O=IBM,C=US\" -x509version 3 -expire 365 -sig_alg SHA1WithRSA\nrunmqakm -cert -extract -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.arm\nrunmqakm -cert -export -db /MQHA/qmgrs/QM_minikube_curam/ssl/key.kdb -stashed -label ibmwebspheremqqm_minikube_curam -target /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -target_type pkcs12 -target_pw Passw0rd\n```\n\n```shell\nopenssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -nocerts -nodes | sed -ne '/-BEGIN PRIVATE KEY-/,/-END PRIVATE KEY-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.key\nopenssl pkcs12 -in /MQHA/qmgrs/QM_minikube_curam/ssl/key_QM_minikube_curam.p12 -passin pass:Passw0rd -clcerts -nokeys | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > /MQHA/qmgrs/QM_minikube_curam/ssl/tls.crt\n```\n\n```shell\nrunmqsc QM_minikube_curam <<-EOS\nALTER QMGR CONNAUTH('CHECK.PWD')\nDEFINE AUTHINFO('CHECK.PWD') AUTHTYPE(IDPWOS) CHCKLOCL(OPTIONAL) CHCKCLNT(OPTIONAL)\nREFRESH SECURITY TYPE(SSL)\nREFRESH SECURITY TYPE(AUTHSERV)\nREFRESH SECURITY TYPE(CONNAUTH)\nEOS\n```\n\nAfter these stages have been run MQ should be configured.\n\n### Clean up QMs/channels/listeners\n\nUsed these steps if you are reconfiguring MQ or cleaning up MQ.\n\nOn both MQ nodes run the following commands:\n\n```shell\nendmqm -w QM_minikube_curam\ndltmqm QM_minikube_curam\nrmvmqinf QM_minikube_curam\n```\n\nOn either MQ node run the following commands:\n\n```shell\nrm -rf /MQHA/qmgrs/**\nrm -rf /MQHA/logs/**\nrm -rf /MQHA/scratch\nendmqm -w QM_minikube_curam\ndltmqm QM_minikube_curam\nrmvmqinf QM_minikube_curam\n```\n\n### MQ on OpenShift\n\n#### Stateful Sets\n\nIf a highly available MQ cluster is desired, a **Stateful Set** can be used. The stateful set used for SPM contains two identical\npods, one active pod and one standby pod. If the active pod goes down, the standby pod is moved into the active role and a new pod is rescheduled in standby mode.\nThis occurs seamlessly, with persistent storage allowing for minimal downtime. The Stateful Set used in SPM requires several values that must be configured prior to\ndeployment. These values are those located under the MQ `multiInstance` section within the relevant values file. There, NFS or Ceph can be chosen as the desired\nmulti-instance delivery method.\n\n* **NFS** - In order to deploy with NFS, an NFS server and NFS folder must be available and configured; the supplementalGroups may need to be provided depending on the NFS server security setup.\n* **Ceph** - In order to deploy with Ceph, the desired Storage Class must be provided.\n\n#### Persistent Volumes & Persistent Volume Claims\n\nA **PersistentVolume** (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes.\nIt is a resource in the cluster just like a node is a cluster resource. A **PersistentVolumeClaim** (PVC) is a request for storage by a user. It is similar to\na Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific\nsize and access modes.\n\nWhen using NFS as the desired multi-instance method, the PV and PVCs must be configured by the user. Within the PVs, the NFS IP and NFS folder must be provided.\nIn the PV, a `claimRef` can be defined in order to ensure that the correct PVC matches with the correct PV. The templates provided also contain labels, which can\nalso be used to ensure correct coupling.\n\nIf using Ceph, the PVs are dynamically configured. Therefore, no further configuration is required.\n","fileAbsolutePath":"/home/travis/build/IBM/spm-kubernetes/src/pages/07-supporting-infrastructure/MQSetupConfig.mdx"}}}}